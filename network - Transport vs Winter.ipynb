{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadb3e23-b515-4c33-967b-629b21b89cb3",
   "metadata": {},
   "source": [
    "# I. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "84f864a2-9ab9-47ea-9d46-bbe654d0f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will import all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import json\n",
    "import re\n",
    "from shapely.geometry import Point, LineString #this library is for manipulating geometric objects, and it is what geopandas uses to store geometries\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7963adbb-cc05-43c6-95ce-9cf332be9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tlf provides passenger flow data for each network separately\n",
    "#we need to create a list of all the different files we need\n",
    "df_dlr = pd.read_csv('https://www.dropbox.com/scl/fi/xkf9ttbop2wyypp8bttgw/NBT19MTT2b_od__DLR_tb_wf.csv?rlkey=me22ozynt7i13ah9hvh9f157m&dl=1')\n",
    "df_ezl = pd.read_csv('https://www.dropbox.com/scl/fi/vizzmmz5u1mhhxignbwei/NBT19MTT2b_od__EZL_tb_wf.csv?rlkey=nz69dj4n7x02389gl7gc95knw&dl=1')\n",
    "df_lo = pd.read_csv('https://www.dropbox.com/scl/fi/rhofj793lzlh6ptheo0ui/NBT19MTT2b_od__LO_tb_wf.csv?rlkey=btj1o5i02dx9paqhpkhk1chlo&dl=1')\n",
    "df_lu = pd.read_csv('https://www.dropbox.com/scl/fi/572yutkebg3pa4pjn42d9/NBT19MTT2b_od__LU_tb_wf.csv?rlkey=0rfklc4u7nxcwbtsjsko0i3xd&dl=1')\n",
    "\n",
    "files = [df_dlr, df_ezl, df_lo, df_lu]\n",
    "london_OD = pd.concat(files, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aea88969-dec9-451f-8936-4abaa0e53539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode_mnlc_o</th>\n",
       "      <th>mode_mnlc_d</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>mode</th>\n",
       "      <th>1</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>504</td>\n",
       "      <td>523</td>\n",
       "      <td>8.03300</td>\n",
       "      <td>16.430</td>\n",
       "      <td>24.563</td>\n",
       "      <td>28.555</td>\n",
       "      <td>13.023</td>\n",
       "      <td>2.263</td>\n",
       "      <td>DLR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>504</td>\n",
       "      <td>533</td>\n",
       "      <td>30.71900</td>\n",
       "      <td>40.105</td>\n",
       "      <td>61.571</td>\n",
       "      <td>57.881</td>\n",
       "      <td>15.573</td>\n",
       "      <td>0.317</td>\n",
       "      <td>DLR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>504</td>\n",
       "      <td>538</td>\n",
       "      <td>5.94100</td>\n",
       "      <td>17.238</td>\n",
       "      <td>37.493</td>\n",
       "      <td>43.158</td>\n",
       "      <td>33.890</td>\n",
       "      <td>10.429</td>\n",
       "      <td>DLR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>504</td>\n",
       "      <td>559</td>\n",
       "      <td>5.79500</td>\n",
       "      <td>26.397</td>\n",
       "      <td>27.285</td>\n",
       "      <td>24.065</td>\n",
       "      <td>15.213</td>\n",
       "      <td>8.508</td>\n",
       "      <td>DLR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>504</td>\n",
       "      <td>573</td>\n",
       "      <td>77.78600</td>\n",
       "      <td>113.750</td>\n",
       "      <td>143.938</td>\n",
       "      <td>387.562</td>\n",
       "      <td>165.776</td>\n",
       "      <td>65.283</td>\n",
       "      <td>DLR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62575</th>\n",
       "      <td>884</td>\n",
       "      <td>784</td>\n",
       "      <td>97.99600</td>\n",
       "      <td>538.180</td>\n",
       "      <td>298.849</td>\n",
       "      <td>173.856</td>\n",
       "      <td>51.263</td>\n",
       "      <td>14.653</td>\n",
       "      <td>LU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62576</th>\n",
       "      <td>884</td>\n",
       "      <td>787</td>\n",
       "      <td>115.91900</td>\n",
       "      <td>281.952</td>\n",
       "      <td>149.409</td>\n",
       "      <td>138.607</td>\n",
       "      <td>64.885</td>\n",
       "      <td>25.433</td>\n",
       "      <td>LU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62577</th>\n",
       "      <td>884</td>\n",
       "      <td>788</td>\n",
       "      <td>139.04400</td>\n",
       "      <td>682.340</td>\n",
       "      <td>768.051</td>\n",
       "      <td>504.189</td>\n",
       "      <td>345.944</td>\n",
       "      <td>113.663</td>\n",
       "      <td>LU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62578</th>\n",
       "      <td>884</td>\n",
       "      <td>789</td>\n",
       "      <td>156.23601</td>\n",
       "      <td>481.302</td>\n",
       "      <td>702.600</td>\n",
       "      <td>595.686</td>\n",
       "      <td>317.113</td>\n",
       "      <td>63.273</td>\n",
       "      <td>LU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62579</th>\n",
       "      <td>884</td>\n",
       "      <td>852</td>\n",
       "      <td>283.30700</td>\n",
       "      <td>1191.298</td>\n",
       "      <td>809.155</td>\n",
       "      <td>366.224</td>\n",
       "      <td>177.616</td>\n",
       "      <td>34.625</td>\n",
       "      <td>LU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62580 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mode_mnlc_o  mode_mnlc_d          2         3        4        5  \\\n",
       "0              504          523    8.03300    16.430   24.563   28.555   \n",
       "1              504          533   30.71900    40.105   61.571   57.881   \n",
       "2              504          538    5.94100    17.238   37.493   43.158   \n",
       "3              504          559    5.79500    26.397   27.285   24.065   \n",
       "4              504          573   77.78600   113.750  143.938  387.562   \n",
       "...            ...          ...        ...       ...      ...      ...   \n",
       "62575          884          784   97.99600   538.180  298.849  173.856   \n",
       "62576          884          787  115.91900   281.952  149.409  138.607   \n",
       "62577          884          788  139.04400   682.340  768.051  504.189   \n",
       "62578          884          789  156.23601   481.302  702.600  595.686   \n",
       "62579          884          852  283.30700  1191.298  809.155  366.224   \n",
       "\n",
       "             6        7 mode    1    8  \n",
       "0       13.023    2.263  DLR  NaN  NaN  \n",
       "1       15.573    0.317  DLR  NaN  NaN  \n",
       "2       33.890   10.429  DLR  NaN  NaN  \n",
       "3       15.213    8.508  DLR  NaN  NaN  \n",
       "4      165.776   65.283  DLR  NaN  NaN  \n",
       "...        ...      ...  ...  ...  ...  \n",
       "62575   51.263   14.653   LU  0.0  0.0  \n",
       "62576   64.885   25.433   LU  0.0  0.0  \n",
       "62577  345.944  113.663   LU  0.0  0.0  \n",
       "62578  317.113   63.273   LU  0.0  0.0  \n",
       "62579  177.616   34.625   LU  0.0  0.0  \n",
       "\n",
       "[62580 rows x 11 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "london_OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fa614e02-6cc5-49d0-8956-69869937c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_df = pd.read_excel('https://www.dropbox.com/scl/fi/8rvd82qd4ald2yyt2bz6m/NBT19_Definitions.xlsx?rlkey=5jcia5kufnlmictzcs9nc2qia&dl=1', 'Stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a50304f4-f53e-4343-9d27-82b799f3b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will drop any missing values from our dataframe\n",
    "definition_df.dropna(inplace=True)\n",
    "\n",
    "#transform mnlc codes from floats to int\n",
    "definition_df.MNLC = definition_df.MNLC.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1e3ddb09-5c03-4d1a-9f1f-17203198baeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#add the station names\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m london_OD[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_origin\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mlondon_OD\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmode_mnlc_o\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefinition_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdefinition_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMNLC\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStationName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m london_OD[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_destination\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m london_OD[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode_mnlc_d\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: definition_df[definition_df\u001b[38;5;241m.\u001b[39mMNLC\u001b[38;5;241m==\u001b[39mx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStationName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/series.py:4765\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4765\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/apply.py:1201\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/apply.py:1281\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1286\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning a DataFrame from Series.apply when the supplied function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns a Series is deprecated and will be removed in a future \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1292\u001b[0m     )  \u001b[38;5;66;03m# GH52116\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/algorithms.py:1812\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1810\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1815\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1816\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[127], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#add the station names\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m london_OD[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_origin\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m london_OD[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode_mnlc_o\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mdefinition_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdefinition_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMNLC\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStationName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m london_OD[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_destination\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m london_OD[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode_mnlc_d\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: definition_df[definition_df\u001b[38;5;241m.\u001b[39mMNLC\u001b[38;5;241m==\u001b[39mx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStationName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3887\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3885\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[1;32m   3886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m-> 3887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3889\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[1;32m   3890\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[1;32m   3891\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3945\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3941\u001b[0m \u001b[38;5;66;03m# check_bool_indexer will throw exception if Series key cannot\u001b[39;00m\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;66;03m# be reindexed to match DataFrame rows\u001b[39;00m\n\u001b[1;32m   3943\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[0;32m-> 3945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3948\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/numpy/core/_methods.py:64\u001b[0m, in \u001b[0;36m_all\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_all\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m umr_all(a, axis, dtype, out, keepdims)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_all(a, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#add the station names\n",
    "london_OD['station_origin'] = london_OD['mode_mnlc_o'].apply(lambda x: definition_df[definition_df.MNLC==x]['StationName'].values[0])\n",
    "london_OD['station_destination'] = london_OD['mode_mnlc_d'].apply(lambda x: definition_df[definition_df.MNLC==x]['StationName'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67123cfb-386b-42cb-a841-85a1a51f0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will only keep passenger flows for the timeband we are interested in\n",
    "london_OD_AMpeak = london_OD[['station_origin', 'station_destination', '3']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663556f-d1f6-43a1-aa07-8a3ab3624dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column\n",
    "london_OD_AMpeak.rename({'3':'flows'}, axis=1, inplace=True)\n",
    "\n",
    "#since the flows are averages the are stored as flows.\n",
    "#for our analysis we will turn the into ints\n",
    "london_OD_AMpeak.flows = london_OD_AMpeak.flows.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87afb99-2230-42ea-b641-3eaa18671c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_origin = list(london_OD_AMpeak.station_origin.values)\n",
    "station_destination = list(london_OD_AMpeak.station_destination.values)\n",
    "all_stations = list(set(station_origin + station_destination))\n",
    "all_stations = sorted(all_stations, key=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254e37b-6fa6-46b1-ac57-85d76bc33ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_OD_AMpeak['station_origin']= london_OD_AMpeak.station_origin.apply(lambda x: \n",
    "                                      re.sub('\\sLU\\s?|\\sLO\\s?|\\sNR\\s?|\\sTf[lL]\\s?|\\sDLR\\s?|\\s\\(.*\\)', '', x)\n",
    "                                     )\n",
    "london_OD_AMpeak['station_destination']=london_OD_AMpeak.station_destination.apply(lambda x: \n",
    "                                      re.sub('\\sLU\\s?|\\sLO\\s?|\\sNR\\s?|\\sTf[lL]\\s?|\\sDLR\\s?|\\s\\(.*\\)', '', x)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607f289-2a79-4532-9491-34c4627ed7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by origin and destination station and sum the flows\n",
    "london_OD_AMpeak = london_OD_AMpeak.groupby(['station_origin', 'station_destination'], as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d546719-2fcc-4ec8-98ec-0b9e11187ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's sanity check and see the highest and lowest flows in and out flows\n",
    "outflows = london_OD_AMpeak.groupby('station_origin', as_index=False).sum()\n",
    "#let's sanity check and see the highest and lowest flows in and out flows\n",
    "inflows = london_OD_AMpeak.groupby('station_destination', as_index=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f713a-1742-438b-9275-223d3e4b4050",
   "metadata": {},
   "source": [
    "### Load London Underground shapefile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e350682-d4f5-4651-9882-4e7d474ba4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data using geopandas\n",
    "stations = gpd.read_file('https://www.dropbox.com/scl/fi/j1ugr3bsub9jhp8b0btkq/tfl_stations.json?rlkey=r38t2xn9n5erc1y3jzub6tqol&dl=1')\n",
    "lines = gpd.read_file('https://www.dropbox.com/scl/fi/ov98qi02qju6kcq5vxnbb/tfl_lines.json?rlkey=h9wip7f1drqn4prgf0fhkcvna&dl=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91f6b4-4835-42f7-8e2b-1209d04543bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac9254-170b-4c64-a375-57a669fdb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project data to British National Grid\n",
    "#we do this so we can work in meters\n",
    "stations = stations.to_crs(epsg = 27700)\n",
    "lines = lines.to_crs(epsg = 27700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b647c3-6942-424f-9725-9b6c01e10bec",
   "metadata": {},
   "source": [
    "### Clean station names\n",
    "\n",
    "We have multiple points representing one station (for example Paddington has seperate points for different entrances). We will simplify this by using RegEx make the names consistent. \n",
    "\n",
    "Once we have cleaned the names we can set the coordinate of these stations to the mean values of the coordinates of all the stations with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2fcd89-496a-4a5b-8821-7b0d5d70492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all station names\n",
    "station_names = {row.id: row['name'] for i, row in stations.iterrows()}\n",
    "\n",
    "#clean station names\n",
    "for k, v in station_names.items():\n",
    "    if k == 'none':\n",
    "        continue\n",
    "    cleaned_name = re.sub('\\sLU\\s?|\\sLO\\s?|\\sNR\\s?|\\sTf[lL]\\s?|\\sDLR\\s?|\\s\\(.*\\)', '', v)\n",
    "    \n",
    "    #note that bank and monument are enconded as one station in our TFL data, so we will do the same\n",
    "    if cleaned_name in ['Bank', 'Monument']:\n",
    "        cleaned_name = 'Bank and Monument'\n",
    "    station_names[k] = cleaned_name\n",
    "    \n",
    "stations['name'] = stations['id'].apply(lambda x: station_names[x])\n",
    "\n",
    "#stations[['name','id']].groupby('name').count().sort_values(by='id',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f390c-e7bc-475f-bb8f-e7f86d07e8f2",
   "metadata": {},
   "source": [
    "**Change position of station to mean of all station with the same name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1602f4b1-e32b-41aa-82a0-0144a2377604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get position of stations as mean of x, y of stations\n",
    "\n",
    "#create empty dict to store list of coordinates\n",
    "pos = {}\n",
    "\n",
    "#iterate through the stations\n",
    "for i, station in stations.iterrows():\n",
    "    xy = station.geometry.coords[0]\n",
    "    \n",
    "    #if station already in dict add coordinate to coordinate list\n",
    "    if station['name'] in pos.keys():\n",
    "        pos[station['name']].extend([xy])\n",
    "    #if station is not in the dict add station to dict and set first coordinate in list\n",
    "    else:\n",
    "        pos[station['name']] = [xy]\n",
    "\n",
    "#iterate through our dict and replace coordinate list with mean value\n",
    "for k, v in pos.items():\n",
    "    #we set axis to 0 to make sure to take mean of x and y coordinates\n",
    "    pos[k] = np.mean(v, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25907dc3-133a-44b4-994d-9b1e13d61f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new geometry\n",
    "stations['geometry'] = stations['name'].apply(lambda x: Point(pos[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af63ab3-9da2-4575-bbf7-30c0daae255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bd6b8-8d6c-456c-b4d2-a183a6cb3802",
   "metadata": {},
   "source": [
    "### Lines file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92511eda-1bf8-4e13-9ff4-c5b48e596052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all line types as a list and subset geoms by lines\n",
    "line_names  = lines.lines.apply(lambda x: [x['name'] for x in json.loads(x)] )\n",
    "line_names = list(set([item for sublist in line_names for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c271f80-100f-45e5-bf3c-6c2330d80255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see that east london line is incomplete\n",
    "#digging into this however we notice that it's because east london is actually part of the overground\n",
    "#merge East London line and London Overground since they are one line\n",
    "lines.lines = lines.lines.str.replace('East London', 'London Overground')\n",
    "stations.lines = stations.lines.str.replace('East London', 'London Overground')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0efa2b-ab53-4fec-90ff-ccfce3a24db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our TFL data doesn't include tramlink, emirates air lines, and crossrail is still in construction (except not really)\n",
    "#exclude the lines that we are not going to use\n",
    "excluded_lines = ['Thameslink 6tph line', 'East London', 'Crossrail 2', 'Emirates Air Line', 'Crossrail', 'Tramlink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97a7fc-2f27-440e-93cb-6022f0ce2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78cc19-c915-473d-b412-459e74eb17d3",
   "metadata": {},
   "source": [
    "## Create seperate graphs for each line and then join\n",
    "\n",
    "Since we don't have properly constructed line geometries because:\n",
    "* there are discontinuities in the line geometries\n",
    "* a line between two stations can actually be composed of more than one geometry\n",
    "\n",
    "We will construct geometric graphs (meaning graphs just from the line geometries where nodes are coordinates that define the line geometry). It's easy to fix the discontinuities in this manner because the can be thought of as nodes with degree = 1 that have another node with degree = 1 within a threshold distance. \n",
    "\n",
    "Once we have these discontinuties fixed we can use these geometric graphs to construct proper geometries between stations by taking the shortest paths within these networks starting from the node closest to the origin station and ending in the node closest to the destination station. The shortest path will then be a list of coordinates that define the line geometry between the two stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af94e09-4c04-4259-a38c-28238676d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty dict to hold our geometric graphs\n",
    "geometric_graphs = {}\n",
    "\n",
    "#iterate through each line individually\n",
    "for line_name in line_names:\n",
    "    #skip lines that we won't be using\n",
    "    if line_name in excluded_lines :\n",
    "        continue\n",
    "    print(line_name)\n",
    "    \n",
    "    #subset our line and station files for the ones the belong to a specific line\n",
    "    temp_lines = lines[lines.lines.str.contains(line_name)]\n",
    "    temp_stations = stations[stations.lines.str.contains(line_name)]\n",
    "    \n",
    "    #create empty graph\n",
    "    G= nx.Graph()\n",
    "    \n",
    "    #iterate through our line geometries\n",
    "    for i, line in temp_lines.iterrows():\n",
    "        #get list of coordinates that define our line\n",
    "        _l = list(line.geometry.coords)\n",
    "\n",
    "        #add coordinates as edges\n",
    "        G.add_edges_from(list(zip(_l,_l[1:])))\n",
    "    \n",
    "    #remove any self loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    \n",
    "    #the position of the node is the name of the node\n",
    "    #let's extract this to plot our graph\n",
    "    pos = {n: n for n in G.nodes()}\n",
    "    \n",
    "    # get degree one nodes\n",
    "    D = nx.degree(G)\n",
    "    k1 = [node for node,degree in D if degree==1]\n",
    "    #let's plot our graph and all degree one nodes\n",
    "    nx.draw(G, pos=pos, node_size=0.5, node_color='grey')\n",
    "    nx.draw_networkx_nodes(G, pos=pos, nodelist=k1, node_color='red', node_size=8)\n",
    "    plt.show()\n",
    "    \n",
    "    #get distance between all points of 1-degree, this will be a matrix\n",
    "    dist = distance.cdist(k1, k1)\n",
    "    \n",
    "    #add edges between points that are close\n",
    "    for j in range(dist.shape[0]-1):\n",
    "        temp = dist[j][j+1:]\n",
    "        #get index of closest node\n",
    "        i_min = np.argmin(temp)\n",
    "        if dist[j][i_min+j+1]<50:\n",
    "            G.add_edge(k1[j],k1[i_min+j+1])\n",
    "    \n",
    "    #recalculate degree\n",
    "    D = nx.degree(G)\n",
    "    k1 = [node for node,degree in D if degree==1]\n",
    "    #plot new graph\n",
    "    nx.draw(G, pos=pos, node_size=0.5, node_color='grey')\n",
    "    nx.draw_networkx_nodes(G, pos=pos, nodelist=k1, node_color='red', node_size=8)\n",
    "    plt.show()\n",
    "    print(nx.number_connected_components(G))\n",
    "    #store graph in dictionary\n",
    "    geometric_graphs[line_name] = G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5d6a0-0213-40d2-ae97-0670e2e95cb3",
   "metadata": {},
   "source": [
    "Now that we have these geometric graphs we can create our final graph by using the data in the lines attribute in our lines geodataframe that contains origin and destination station. We can then use our geometric graph and find the shortest route to create our line geometries and find the length of the lines. \n",
    "\n",
    "We will store this information in a dataframe that we can use to create our final graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb1732e-b1a6-426a-a47c-7afe34d53abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with all geometries from all valid lines\n",
    "#this list will contain a dictionary for each edge in our graph that contains\n",
    "# node pairs, line name, and a geometry (this is just for quick visualization purposes and will be replaced by true geometry)\n",
    "edge_list = []\n",
    "def _has_ids(k):\n",
    "    #helper function to check if line contains id of start and end station\n",
    "    if ('start_sid' in k.keys()) and ('end_sid' in k.keys()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#iterate through each line seperately \n",
    "for line_name in line_names:\n",
    "    if line_name in excluded_lines :\n",
    "        continue\n",
    "        \n",
    "    #subset lines and stations\n",
    "    temp_lines = lines[lines.lines.str.contains(line_name)]\n",
    "    temp_stations = stations[stations.lines.str.contains(line_name)]\n",
    "    \n",
    "    #for each line in our subset dataframe get each origin and destination pair\n",
    "    for i, line in temp_lines.iterrows():\n",
    "        for k in json.loads(line.lines):\n",
    "            if k['name'] == line_name:\n",
    "                if _has_ids(k):\n",
    "                    #get start and end station ids\n",
    "                    start_id = k['start_sid']\n",
    "                    end_id = k['end_sid']\n",
    "\n",
    "                    #get info of start and end station\n",
    "                    start_station = temp_stations[(temp_stations.id == start_id) | (temp_stations.altmodeid == start_id)].values\n",
    "                    end_station = temp_stations[(temp_stations.id == end_id) | (temp_stations.altmodeid == end_id)].values\n",
    "                    \n",
    "                    #if no matching station exist, let's just take the starting and ending coordinate of the line for now\n",
    "                    if len(start_station) >= 1 and len(end_station) >=1:\n",
    "                        s_geom = start_station[0][-1].coords[0]\n",
    "                        e_geom = end_station[0][-1].coords[0]\n",
    "                        edge_list.append({\n",
    "                            'line_name': line_name,\n",
    "                            'start_id': start_id,\n",
    "                            'end_id': end_id,\n",
    "                            'geometry': LineString([s_geom, e_geom])\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492137b-359a-45a6-ba59-b9a0ec3ba295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn our dictionary list into a geopandas df to quickly plot and sanity check our work\n",
    "edge_gdf = gpd.GeoDataFrame(edge_list)\n",
    "edge_gdf.crs = lines.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d7046-619f-4744-ba45-e075d56481d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add geometry information back into the graph\n",
    "for line_name in line_names:\n",
    "    if line_name in excluded_lines :\n",
    "        continue\n",
    "    temp_lines = edge_gdf[edge_gdf.line_name == line_name]\n",
    "    temp_stations = stations[stations.lines.str.contains(line_name)]\n",
    "    \n",
    "    #let's plot our lines so we can check them\n",
    "    fig, ax = plt.subplots(figsize = (7,7))\n",
    "    fig.suptitle(f'line: {line_name}')\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis('off')\n",
    "    ax.margins(0.1)\n",
    "    \n",
    "    base = temp_lines.plot(ax=ax, color='grey')\n",
    "    temp_stations.plot(ax=base, markersize=20, color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    #add real geometry back\n",
    "    for i, edge in temp_lines.iterrows():\n",
    "        #get start and end of line coordinates\n",
    "        start = edge.geometry.coords[0]\n",
    "        end = edge.geometry.coords[-1]\n",
    "        \n",
    "        #let's get all the coordinates in our geometric graph\n",
    "        nodes = list(geometric_graphs[line_name].nodes)\n",
    "        \n",
    "        #find nearest node in graph to origin and destination\n",
    "        s_dist = distance.cdist([start], nodes)[0]\n",
    "        s_i = np.argmin(s_dist)\n",
    "        source= nodes[s_i]\n",
    "        \n",
    "        t_dist = distance.cdist([end], nodes)[0]\n",
    "        t_i = np.argmin(t_dist)\n",
    "        target= nodes[t_i]\n",
    "        \n",
    "        #get shortest path\n",
    "        sp = nx.shortest_path(geometric_graphs[line_name], source, target)\n",
    "        \n",
    "        #make into geometry\n",
    "        #notice how I'm also adding the start and end coordinates to the line definition\n",
    "        #this fixes the problem of stations and lines not matching up\n",
    "        geometry = LineString([start] + sp + [end])\n",
    "        edge_gdf.loc[i, 'geometry'] = geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75484ee9-c5c8-4c92-bf2a-1ca75ed8a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's store the length of the real geometry\n",
    "edge_gdf['length'] = edge_gdf.geometry.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1a0cd-c09b-4371-b0b7-62484704f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's replace the ids with the actual name\n",
    "#get all station names\n",
    "station_names_2 = {row.altmodeid: row['name'] for i, row in stations.iterrows() if row.altmodeid != None}\n",
    "station_names.update(station_names_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ccc07-c58f-4873-8b0a-991ed1a3e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_gdf['start_id'] = edge_gdf['start_id'].apply(lambda x: station_names[x])\n",
    "edge_gdf['end_id'] = edge_gdf['end_id'].apply(lambda x: station_names[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4cf1f2-a3ff-4356-8049-d62681650795",
   "metadata": {},
   "source": [
    "## IMPORTANT!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1646b0-0c8e-42a4-acd1-7f75293640f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can use this to create our network!\n",
    "G = nx.from_pandas_edgelist(edge_gdf, source = 'start_id', target='end_id', edge_attr=['length', 'line_name'])\n",
    "\n",
    "#let's plot\n",
    "#notice that even though we created the real geometries, the graph still plots just straight line\n",
    "#this is because networkx doesn't have a concept of a edge geometry\n",
    "#however this is ok, since we only need the distance as weights, and that is already an attribute in our graph\n",
    "pos = {row['name']: row.geometry.coords[0] for i, row in stations.iterrows() if row['name'] in G.nodes()}\n",
    "Gcc = nx.connected_components(G)\n",
    "for n in Gcc:\n",
    "    G_sub = G.subgraph(n)\n",
    "    lines = [data['line_name'] for u,v, data in G_sub.edges(data=True)]\n",
    "    print(set(lines))\n",
    "    nx.draw(G_sub, pos, node_size=4)\n",
    "    plt.show()\n",
    "\n",
    "#save position to graph\n",
    "nx.set_node_attributes(G, pos, 'coords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc702c-0b3e-4921-919e-c805ddcfa951",
   "metadata": {},
   "source": [
    "### Sanity check our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69189b59-f924-4a54-b22a-e1bd345e2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that all distances are greater or equal to euclidean distance\n",
    "for u,v, data in G.edges(data=True):\n",
    "    assert(data['length'] >= distance.euclidean(pos[u], pos[v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e458f-2bc1-448b-b98c-43ad416060d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if want to know what is the maximum distance between stations\n",
    "max(dict(G.edges).items(), key=lambda x: x[1]['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f504851-8347-404f-a1b9-c8c389c75d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if want to know to which stations Baker Street is directly connected to\n",
    "Baker_Street = [(u,v) for  u,v in G.edges() if u == 'Baker Street' or v == 'Baker Street']\n",
    "Baker_Street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af483113-da73-4aa2-899a-423e4611a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also check the degree of the nodes in our network and check that they make sense\n",
    "deg_london = nx.degree(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d35644-0427-4eef-b601-656fcfc9a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index = dict(deg_london).keys())\n",
    "df['degree'] = dict(deg_london).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626a40f-78be-48e6-9d6f-6c15bffb6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('degree', ascending =False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad177e5-1844-4f91-b0be-8697cd8ab061",
   "metadata": {},
   "source": [
    "### Merge Network with OD data\n",
    "\n",
    "ok, we are almost done!\n",
    "\n",
    "We have processed our raw data, now we need to combine the two (TLF OD and our Network). For this we need two types of merge:\n",
    "\n",
    "1. add flows as weights to the network (to be able to calculate disruptions to the network)\n",
    "2. create OD with distance for our spatial interaction models (for our spatial interaction models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e588db5-7b3d-455a-87ec-7761b776b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that our OD data and network data match\n",
    "OD_names = set(london_OD_AMpeak.station_origin.unique())\n",
    "network_names = set([n for n in G.nodes()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43be73e-0941-4b6a-82ae-2170547d9a39",
   "metadata": {},
   "source": [
    "Ok, we have a few stations that don't match up. They are small enough that we can solve this quickly by creating a mapping between names of the stations that we can keep.\n",
    "\n",
    "* Battersea power station and Nine Elms data doesn't exist in our TFL data because the station wasn't opened when the data was collected. This was an extension of Northern line that opened recently.\n",
    "* Action Main Line, Hanwell, Hayes & Harlington, Southall, and West Ealing are part of crossrail which we are not considering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075452b4-dfdc-4049-9508-6481f20f7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = {\n",
    "    'Heathrow Terminal 4 EL': 'Heathrow Terminal 4',\n",
    "    'Heathrow Terminals 123': 'Heathrow Terminals 2 & 3',\n",
    "    'Heathrow Terminals 2 & 3 EL': 'Heathrow Terminals 2 & 3',\n",
    "    \"Walthamstow Queen's Road\": 'Walthamstow Queens Road'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09164cb0-9063-48cc-a733-78eec55750c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_OD_AMpeak.station_origin = london_OD_AMpeak.station_origin.apply(\n",
    "    lambda x: name_map[x] if x in name_map.keys() else x\n",
    ")\n",
    "london_OD_AMpeak.station_destination = london_OD_AMpeak.station_destination.apply(\n",
    "    lambda x: name_map[x] if x in name_map.keys() else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a8a55-fb31-462d-821e-9a5ceeb489a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's filter out the stations that we don't have in our network\n",
    "OD_names = set(london_OD_AMpeak.station_origin.unique())\n",
    "_filter = list(network_names.symmetric_difference(OD_names))\n",
    "_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696d549-dc1a-41eb-956c-67d2200a6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_OD_AMpeak = london_OD_AMpeak[~london_OD_AMpeak.station_origin.isin(_filter)]\n",
    "london_OD_AMpeak = london_OD_AMpeak[~london_OD_AMpeak.station_destination.isin(_filter)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c289b2-a19c-439d-874b-ae24c464d172",
   "metadata": {},
   "source": [
    "**Add flow data to our network**\n",
    "\n",
    "Our TFL data contains flows for OD pairs, but we don't know the flows passing through each edge in our network. We will have to calculate this assuming fall people travelling from Origin to Destination station are taking the shortest path within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1142e-1ae3-41f9-b10f-ad79e4fbaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary to store flows for all edges\n",
    "flows = {(u,v): 0 for u,v in G.edges()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab5067-69ae-45b6-b6dc-ae9517edffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate shortest paths for all flows and add data to dict\n",
    "for i, row in london_OD_AMpeak.iterrows():\n",
    "    source = row.station_origin\n",
    "    target = row.station_destination\n",
    "    \n",
    "    #get shortest path\n",
    "    path = nx.dijkstra_path(G, source, target)\n",
    "    \n",
    "    #our path is a list of nodes, we need to turn this to a list of edges\n",
    "    path_edges = list(zip(path,path[1:])) \n",
    "    \n",
    "    #add flows to our dict\n",
    "    for u,v in path_edges:\n",
    "        try:\n",
    "            flows[(u,v)] += row.flows\n",
    "        except:\n",
    "            flows[(v,u)] += row.flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ab0b0-dfbc-49d7-bd2c-667bcc3e43f3",
   "metadata": {},
   "source": [
    "### IMPORTANT!!! (INTEGRATION BETWEEN NETWORK AND WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e338e-a74d-419c-bfb1-bd657af43921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumption, uniform speed x different distance = time\n",
    "#time = distance / speed\n",
    "#speed = 33kmph or 9mps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3535c30d-9ff8-4b4e-8e53-1d7e820e0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge, data in dict(G.edges).items():\n",
    "    print(f\"Edge: {edge}, Data: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5087ebf-0370-4fd8-abb6-76456333ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'velocity' field to each edge with value 60\n",
    "for u, v, data in G.edges(data=True):\n",
    "    data['velocity'] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ee0d4-2bac-4231-8461-878985ce50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge, data in dict(G.edges).items():\n",
    "    print(f\"Edge: {edge}, Data: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffec33b-3784-4525-bfcb-1db7ea22402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'velocity' field to each edge with value 60\n",
    "for u, v, data in G.edges(data=True):\n",
    "    data['train_time'] = data['length'] / data['velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04888a0-fcd1-4471-8e38-8b5d0e0806a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'velocity' field to each edge with value 60\n",
    "for u, v, data in G.edges(data=True):\n",
    "    data['dwell_time'] = data['flows'] / 50000 * 120\n",
    "    data['travel_time'] = data['dwell_time'] + data['train_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812afef2-5ee7-434f-b819-588c04477d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10efafa9-58fc-43f5-86a9-0d276e2d3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the new weight, time and flows (travel time)\n",
    "#travel_time = f(flows, train_time, dwell_time) # finding the formula\n",
    "#travel_time = train_time + (flows * dwell time)\n",
    "\n",
    "# Estimating Additional Dwell Time for 1,000 Passengers:\n",
    "\n",
    "# If we assume a boarding/alighting time of 2.5 seconds per person (an average in crowded conditions)\n",
    "#dwell_time = 1,000 people * 2.5 seconds/person = 2,500 seconds.\n",
    "    \n",
    "# Considering Multiple Doors\n",
    "# London Underground trains have multiple doors per carriage (typically 3 to 4 doors per side, with about 6 to 8 carriages)\n",
    "#dwell_time = Total boarding time / Number of doors = 2,500 seconds / 20 = 125 seconds (~2 minus for every additional 1000 passengers)\n",
    "\n",
    "#travel_time (A->B) = train_time (A->B) + ((total flows of all stations between A->B /1000) * 2 min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce5129-11bd-474e-bc0f-6d7dd7f88802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputting the new weight into the network\n",
    "#set this as a network attribute (change flow to new weight)\n",
    "#nx.set_edge_attributes(G, flows, 'flows') # <- change into travel_time\n",
    "\n",
    "#plot our calcuated flows\n",
    "max_travel_time = max(data['travel_time'] for u, v, data in G.edges(data=True))\n",
    "travel_color = [(i[2]['travel_time']/max_travel_time) for i in G.edges(data=True)]\n",
    "travel_width = [(i[2]['travel_time']/max_travel_time)*10 for i in G.edges(data=True)]\n",
    "\n",
    "# Plot graph\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "#pos=nx.spring_layout(X)\n",
    "edg=nx.draw_networkx_edges(G, pos,edge_color=travel_color, width=travel_width)\n",
    "\n",
    "nx.draw_networkx_nodes(G,\n",
    "        pos = pos,\n",
    "        node_color= 'black',\n",
    "        node_size= 1)\n",
    "\n",
    "plt.colorbar(edg,label=\"Passenger Travel Time\",orientation=\"horizontal\", shrink=0.5)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"London network Passenger Travel Time\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ea2b5-7e08-43dc-a94f-7eb320f1fbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce7ad0-f402-4c20-b777-7808bd1b547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this as a network attribute (change flow to new weight)\n",
    "nx.set_edge_attributes(G, flows, 'flows') # <- change into travel_time\n",
    "\n",
    "#plot our calcuated flows\n",
    "flows_values = flows.values()\n",
    "flow_color=[(i[2]['flows']/max(flows_values)) for i in G.edges(data=True)]\n",
    "flow_width=[(i[2]['flows']/max(flows_values)*10) for i in G.edges(data=True)]\n",
    "\n",
    "# Plot graph\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "#pos=nx.spring_layout(X)\n",
    "edg=nx.draw_networkx_edges(G, pos,edge_color=flow_color, width=flow_width)\n",
    "\n",
    "nx.draw_networkx_nodes(G,\n",
    "        pos = pos,\n",
    "        node_color= 'black',\n",
    "        node_size= 1)\n",
    "\n",
    "plt.colorbar(edg,label=\"Passenger Flows\",orientation=\"horizontal\", shrink=0.5)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"London network Passenger Flows\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ce95c-ca50-4afc-9eb5-a2de4f14cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted alternatives\n",
    "## 1. time and flow = weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d1b4e-bfd2-4e05-bea6-83674ea924d2",
   "metadata": {},
   "source": [
    "# II. Average Shortest Path Length (APL) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f8475-1e6b-4200-8a82-fcedbd0202a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Scenario 1: Business As Usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013bc41-a488-4544-addd-abf7c3b2198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shortest_path_length = nx.average_shortest_path_length(G, weight='travel_time')\n",
    "print(avg_shortest_path_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c5ce50-e627-42a6-adae-233afb62dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all-pairs shortest paths (weighted)\n",
    "path_lengths = dict(nx.all_pairs_dijkstra_path_length(G, weight='travel_time'))\n",
    "\n",
    "# Extract the shortest path lengths into a list\n",
    "lengths = []\n",
    "for source, target_dict in path_lengths.items():\n",
    "    for target, length in target_dict.items():\n",
    "        if source != target:  # avoid zero-length paths (self loops)\n",
    "            lengths.append(length)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(lengths, bins=5, edgecolor='black')\n",
    "plt.title('Histogram of Shortest Path Lengths (Weighted)')\n",
    "plt.xlabel('Path Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660cf2a4-1be2-4839-9371-0b1270cb5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## b. Scenario 2: Winter Disruption without Government Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85de7f-f2ca-4b69-b1a6-5dec43615504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddabe5b5-2458-4e96-8f4a-0ac28ca444b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## c. Scenario 3: Winter Disruption with Government Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e39892-5d84-43cd-9a81-809f8e21f7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e22c9-2291-4d47-99e0-3a8236c529c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## d. APL Comparison between Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40da350-289f-42ab-9a95-0b9c54d55892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e95bd341-1924-464b-bf44-1e07e403a563",
   "metadata": {},
   "source": [
    "# III. Betweeness Centrality (BC) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2819d6-893b-4e06-b4be-4431e8feec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Scenario 1: Business As Usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579347f-28e6-4c28-ad1e-4fb21668b3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c59ca-bc80-4284-8377-753792b355ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## b. Scenario 2: Winter Disruption without Government Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40a323-01c8-4976-ab07-988eaceb0e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67481b59-a912-4c90-907d-3ee15d69f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## c. Scenario 3: Winter Disruption with Government Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95f34a-8939-4396-afa3-5564b7348e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c4ad4f-1743-4de8-bfc5-5012cc491b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## d. BC Comparison between Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5771e1-b5aa-4cff-b66c-eea11bf4294e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd55f80b-3ee5-480e-94ab-801a73f9e31e",
   "metadata": {},
   "source": [
    "# IV. Intervention Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f9bfc-6cb1-4f37-9a01-de61c233d85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
